{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPAbmQ4DANlg"
      },
      "source": [
        "## 模型架構 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxZpG_QUANlg"
      },
      "source": [
        "### Lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42iXEetaANlg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Util #\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.sparse import linalg\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Layer #\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import numbers\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Model #\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import sys\n",
        "\n",
        "# Trainer #\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Main #\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m9BJMEvANlh"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3lArIYWANlh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataLoaderM(object):\n",
        "    def __init__(self, xs, ys,xs_1,xs_2,xs_3,xs_4, batch_size, pad_with_last_sample=True):\n",
        "        \"\"\"\n",
        "        :param xs:\n",
        "        :param ys:\n",
        "        :param batch_size:\n",
        "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.current_ind = 0\n",
        "\n",
        "        # 將資料長度補齊至batch_size可整除之數量\n",
        "        # 補齊方法: 取原資料最後一個並複製多個來補齊\n",
        "        if pad_with_last_sample:\n",
        "            # 計算需補齊數量\n",
        "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
        "            \n",
        "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
        "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
        "\n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs = np.concatenate([xs, x_padding], axis=0)\n",
        "            ys = np.concatenate([ys, y_padding], axis=0)\n",
        "            \n",
        "            ### MA6 ###\n",
        "            # 計算需補齊數量\n",
        "            x_padding = np.repeat(xs_1[-1:], num_padding, axis=0)\n",
        "            \n",
        "            \n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs_1 = np.concatenate([xs_1, x_padding], axis=0)\n",
        "            \n",
        "            \n",
        "            ### MA36 ###\n",
        "            # 計算需補齊數量\n",
        "            x_padding = np.repeat(xs_2[-1:], num_padding, axis=0)\n",
        "            \n",
        "\n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs_2 = np.concatenate([xs_2, x_padding], axis=0)\n",
        "            \n",
        "            \n",
        "            \n",
        "            ### GA6 ###\n",
        "            # 計算需補齊數量\n",
        "            x_padding = np.repeat(xs_3[-1:], num_padding, axis=0)\n",
        "            \n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs_3 = np.concatenate([xs_3, x_padding], axis=0)\n",
        "            \n",
        "            \n",
        "            ### GA36 ###\n",
        "            # 計算需補齊數量\n",
        "            x_padding = np.repeat(xs_4[-1:], num_padding, axis=0)\n",
        "\n",
        "            # 將複製後的ele進行concatenate以補齊成可整除batch_size之長度\n",
        "            xs_4 = np.concatenate([xs_4, x_padding], axis=0)\n",
        "            \n",
        "            \n",
        "        self.size = len(xs)\n",
        "        self.num_batch = int(self.size // self.batch_size)\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "        \n",
        "        self.xs_1 = xs_1\n",
        "        \n",
        "        self.xs_2 = xs_2\n",
        "        \n",
        "        self.xs_4 = xs_4\n",
        "        \n",
        "        self.xs_3 = xs_3\n",
        "\n",
        "    def shuffle(self):\n",
        "        permutation = np.random.permutation(self.size)\n",
        "        xs, ys = self.xs[permutation], self.ys[permutation]\n",
        "        xs_1 = self.xs_1[permutation] \n",
        "        xs_2 = self.xs_2[permutation] \n",
        "        \n",
        "        \n",
        "        xs_3 = self.xs_3[permutation] \n",
        "        xs_4 = self.xs_4[permutation] \n",
        "        \n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "        \n",
        "        self.xs_1 = xs_1\n",
        "        \n",
        "        \n",
        "        self.xs_2 = xs_2\n",
        "        \n",
        "        \n",
        "        self.xs_4 = xs_4\n",
        "        \n",
        "        self.xs_3 = xs_3\n",
        "\n",
        "    def get_iterator(self):\n",
        "        self.current_ind = 0\n",
        "        def _wrapper():\n",
        "            while self.current_ind < self.num_batch:\n",
        "                start_ind = self.batch_size * self.current_ind\n",
        "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
        "                x_i = self.xs[start_ind: end_ind, ...]\n",
        "                y_i = self.ys[start_ind: end_ind, ...]\n",
        "                \n",
        "                x_i_1 = self.xs_1[start_ind: end_ind, ...]\n",
        "                \n",
        "                \n",
        "                x_i_2 = self.xs_2[start_ind: end_ind, ...]\n",
        "                \n",
        "                \n",
        "                x_i_4 = self.xs_4[start_ind: end_ind, ...]\n",
        "                \n",
        "                x_i_3 = self.xs_3[start_ind: end_ind, ...]\n",
        "                \n",
        "                # 節省記憶體:\n",
        "                # yield 設計來的目的，就是為了單次輸出內容\n",
        "                # 我們可以把 yield 暫時看成 return，但是這個 return 的功能只有單次\n",
        "                # 而且，一旦我們的程式執行到 yield 後，程式就會把值丟出，並暫時停止\n",
        "                yield (x_i, y_i,x_i_1,x_i_2, x_i_3, x_i_4)\n",
        "                self.current_ind += 1\n",
        "\n",
        "        return _wrapper()\n",
        "\n",
        "class StandardScaler():\n",
        "    \"\"\"\n",
        "    Standard the input\n",
        "    \"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "    def transform(self, data):\n",
        "        return (data - self.mean) / self.std\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * self.std) + self.mean\n",
        "'''\n",
        "\n",
        "class StandardScaler():\n",
        "    \"\"\"\n",
        "    Standard the input\n",
        "    \"\"\"\n",
        "    def __init__(self, max, min):\n",
        "        self.max = max\n",
        "        self.min = min\n",
        "    def transform(self, data):\n",
        "        return (data - self.min) / (self.max - self.min)\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * (self.max - self.min) ) + self.min\n",
        "'''\n",
        "\n",
        "def asym_adj(adj):\n",
        "    \"\"\"Asymmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)).flatten()\n",
        "    d_inv = np.power(rowsum, -1).flatten()\n",
        "    d_inv[np.isinf(d_inv)] = 0.\n",
        "    d_mat= sp.diags(d_inv)\n",
        "    return d_mat.dot(adj).astype(np.float32).todense()\n",
        "\n",
        "\n",
        "def load_pickle(pickle_file):\n",
        "    try:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f)\n",
        "    except UnicodeDecodeError as e:\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            pickle_data = pickle.load(f, encoding='latin1')\n",
        "    except Exception as e:\n",
        "        print('Unable to load data ', pickle_file, ':', e)\n",
        "        raise\n",
        "    return pickle_data\n",
        "\n",
        "def load_adj(pkl_filename, adjtype):\n",
        "    sensor_ids, sensor_id_to_ind, adj_mx = load_pickle(pkl_filename)\n",
        "\n",
        "    print('# 全部L.A.的sensor ID(sensor_ids):\\n',sensor_ids)\n",
        "    print('# 將sensor ID對應index(sensor_id_to_ind):\\n',sensor_id_to_ind)\n",
        "    \n",
        "    if adjtype == \"scalap\":\n",
        "        adj = [calculate_scaled_laplacian(adj_mx)]\n",
        "    elif adjtype == \"normlap\":\n",
        "        adj = [calculate_normalized_laplacian(adj_mx).astype(np.float32).todense()]\n",
        "    elif adjtype == \"symnadj\":\n",
        "        adj = [sym_adj(adj_mx)]\n",
        "    elif adjtype == \"transition\":\n",
        "        adj = [asym_adj(adj_mx)]\n",
        "    elif adjtype == \"doubletransition\":\n",
        "        adj = [asym_adj(adj_mx), asym_adj(np.transpose(adj_mx))]   # asym_adj(adj_mx): forward transition matrix / asym_adj(np.transpose(adj_mx)): backward transition matrix\n",
        "    elif adjtype == \"identity\":\n",
        "        adj = [np.diag(np.ones(adj_mx.shape[0])).astype(np.float32)]\n",
        "    else:\n",
        "        error = 0\n",
        "        assert error, \"adj type not defined\"\n",
        "\n",
        "    print('# Double transition Transition matrix of Eq 4:\\n',adj)\n",
        "    return sensor_ids, sensor_id_to_ind, adj\n",
        "\n",
        "def load_dataset(dataset_dir, batch_size, valid_batch_size= None, test_batch_size=None):\n",
        "    data = {}\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\n",
        "        data['x_' + category] = cat_data['x']\n",
        "        data['y_' + category] = cat_data['y']\n",
        "\n",
        "        if args.log_print:\n",
        "            print(\"# category:\", category)\n",
        "            print('x:',data['x_' + category].shape, data['x_' + category][0] )\n",
        "            print('y:',data['y_' + category].shape, data['y_' + category][0] )\n",
        "    \n",
        "    # 使用train的mean/std來正規化valid/test #\n",
        "    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n",
        "    # 將欲訓練特徵改成正規化\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\n",
        "\n",
        "    \n",
        "\n",
        "    data['train_loader'] = DataLoaderM(data['x_train'], data['y_train'], batch_size)\n",
        "    data['val_loader'] = DataLoaderM(data['x_val'], data['y_val'], valid_batch_size)\n",
        "    data['test_loader'] = DataLoaderM(data['x_test'], data['y_test'], test_batch_size)\n",
        "    data['scaler'] = scaler\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def masked_mse(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /= torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = (preds-labels)**2\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def masked_rmse(preds, labels, null_val=np.nan):\n",
        "    return torch.sqrt(masked_mse(preds=preds, labels=labels, null_val=null_val))\n",
        "\n",
        "\n",
        "def masked_mae(preds, labels, null_val=np.nan):\n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = torch.abs(preds-labels)\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "def masked_mape(preds, labels, null_val=np.nan):\n",
        "    \n",
        "    if np.isnan(null_val):\n",
        "        mask = ~torch.isnan(labels)\n",
        "    else:\n",
        "        mask = (labels!=null_val)\n",
        "    mask = mask.float()\n",
        "    mask /=  torch.mean((mask))\n",
        "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
        "    loss = 2.0 * (torch.abs(preds - labels) / (torch.abs(preds) + torch.abs(labels)))\n",
        "    loss = loss * mask\n",
        "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "def metric(pred, real):\n",
        "    mae = masked_mae(pred,real,0.0).item()\n",
        "    mape = masked_mape(pred,real,0.0).item()\n",
        "    rmse = masked_rmse(pred,real,0.0).item()\n",
        "    return mae,mape,rmse\n",
        "\n",
        "# Ref: https://github.com/nnzhan/MTGNN\n",
        "class LayerNorm(nn.Module):\n",
        "    __constants__ = ['normalized_shape', 'weight', 'bias', 'eps', 'elementwise_affine']\n",
        "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        self.normalized_shape = tuple(normalized_shape)\n",
        "        self.eps = eps\n",
        "        self.elementwise_affine = elementwise_affine\n",
        "        if self.elementwise_affine:\n",
        "            self.weight = nn.Parameter(torch.Tensor(*normalized_shape))\n",
        "            self.bias = nn.Parameter(torch.Tensor(*normalized_shape))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.elementwise_affine:\n",
        "            init.ones_(self.weight)\n",
        "            init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, idx):\n",
        "        if self.elementwise_affine:\n",
        "            return F.layer_norm(input, tuple(input.shape[1:]), self.weight[:,idx,:], self.bias[:,idx,:], self.eps)\n",
        "        else:\n",
        "            return F.layer_norm(input, tuple(input.shape[1:]), self.weight, self.bias, self.eps)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{normalized_shape}, eps={eps}, ' \\\n",
        "            'elementwise_affine={elementwise_affine}'.format(**self.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCXijts6VgTW"
      },
      "source": [
        "### F-GMAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWBPDdtjVgTX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class F_GMAT_base(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, bias=True):\n",
        "        super(F_GMAT_base, self).__init__()\n",
        "\n",
        "        print('F_GMAT_base', n_heads, in_channel, num_nodes, dropout)\n",
        "        self.n_head = n_heads\n",
        "        self.f_in = num_nodes\n",
        "        self.a_src = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "        self.a_dst = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_nodes))\n",
        "            nn.init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.a_src, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a_dst, gain=1.414)\n",
        "\n",
        "    def forward(self, h):\n",
        "        bs, ch, n, dim = h.size()\n",
        "        h_prime = h\n",
        "        attn_src = torch.matmul(h, self.a_src)\n",
        "        attn_dst = torch.matmul(h, self.a_dst)\n",
        "        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(\n",
        "            0, 1, 3, 2\n",
        "        )\n",
        "        attn = self.leaky_relu(attn)\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, h_prime)\n",
        "        return output + self.bias, attn\n",
        "        \n",
        "class F_GMAT(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, alpha):\n",
        "        super(F_GMAT, self).__init__()\n",
        "        \n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.layer = F_GMAT_base(\n",
        "                    n_heads, in_channel, num_nodes, dropout\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs,ch,n,dim = x.size()\n",
        "        x, attn = self.layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class F_GMAT_module(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, mlp, mlp2, dropout, alpha):\n",
        "        super(F_GMAT_module, self).__init__()\n",
        "        print('F_GMAT_module', n_heads, in_channel, num_nodes, dropout, alpha)\n",
        "        self.net = F_GMAT(n_heads, in_channel, num_nodes, dropout, alpha)\n",
        "\n",
        "        self.mlp_convs = nn.ModuleList()\n",
        "        self.mlp_bns = nn.ModuleList()\n",
        "        last_channel = 32\n",
        "        for out_channel in mlp:\n",
        "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            last_channel = out_channel\n",
        "        \n",
        "        self.mlp_convs2 = nn.ModuleList()\n",
        "        self.mlp_bns2 = nn.ModuleList()\n",
        "        last_channel = n_heads\n",
        "        for out_channel in mlp2:\n",
        "            self.mlp_convs2.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            last_channel = out_channel\n",
        "\n",
        "        self.lay_norm2 = nn.LayerNorm([n_heads,5, num_nodes])\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.6)\n",
        "    def forward(self,x, x_1, x_2, x_3, x_4):\n",
        "        bs, ch, n, dim = x.size()\n",
        "      \n",
        "        x_all = []\n",
        "        x_1_all = []\n",
        "        x_2_all =[]\n",
        "        x_3_all = []\n",
        "        x_4_all = []\n",
        "\n",
        "        for t_idx in range(1,dim):\n",
        "            x_input = [x[:,:,:,t_idx].unsqueeze(2),\n",
        "                    x_1[:,:,:,t_idx].unsqueeze(2),x_2[:,:,:,t_idx].unsqueeze(2),\n",
        "                    x_3[:,:,:,t_idx].unsqueeze(2),x_4[:,:,:,t_idx].unsqueeze(2)\n",
        "                  ]\n",
        "            x_input = torch.cat(x_input, dim=2)\n",
        "            x_input_cpy = x_input\n",
        "\n",
        "            for i, conv in enumerate(self.mlp_convs):\n",
        "              x_input = F.relu((conv(x_input)))\n",
        "\n",
        "            x_input_cpy2 = x_input\n",
        "            x_input = self.net(x_input)\n",
        "            x_input = x_input_cpy2+ self.dropout1(x_input)\n",
        "\n",
        "            x_input = self.lay_norm2(x_input)\n",
        "            \n",
        "            for i, conv in enumerate(self.mlp_convs2):\n",
        "              x_input = F.relu((conv(x_input)))\n",
        "\n",
        "            x_input = x_input_cpy+ self.dropout2(x_input)\n",
        "\n",
        "            x_all.append(x_input[:,:,0].unsqueeze(3))\n",
        "            \n",
        "            \n",
        "        x_tmp = torch.cat(x_all, dim=3)  # (64,16,207,13)\n",
        "        x = torch.cat([x[:,:,:,:1],x_tmp],dim=3)\n",
        "        \n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEe8TmjaVgTX"
      },
      "source": [
        "### T-GMAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "7M01blgIVgTX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class T_GMAT_base(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, bias=True):\n",
        "        super(T_GMAT_base, self).__init__()\n",
        "\n",
        "        print('T_GMAT_base', n_heads, in_channel, num_nodes, dropout)\n",
        "        self.n_head = n_heads\n",
        "        self.f_in = num_nodes\n",
        "        self.a_src = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "        self.a_dst = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_nodes))\n",
        "            nn.init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.a_src, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a_dst, gain=1.414)\n",
        "\n",
        "    def forward(self, h):\n",
        "        bs, ch, n, dim = h.size()\n",
        "        h_prime = h\n",
        "        attn_src = torch.matmul(h, self.a_src)\n",
        "        attn_dst = torch.matmul(h, self.a_dst)\n",
        "        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(\n",
        "            0, 1, 3, 2\n",
        "        )\n",
        "        attn = self.leaky_relu(attn)\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, h_prime)\n",
        "        return output + self.bias, attn\n",
        "        \n",
        "class T_GMAT(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, alpha):\n",
        "        super(T_GMAT, self).__init__()\n",
        "        \n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.layer = T_GMAT_base(\n",
        "                    n_heads, in_channel, num_nodes, dropout\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs,ch,n,dim = x.size()\n",
        "        x, attn = self.layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class T_GMAT_module(nn.Module):\n",
        "    def __init__(self, kern, dilation_factor, temporal_len, n_heads, in_channel, num_nodes, mlp, mlp2, dropout, alpha):\n",
        "        super(T_GMAT_module, self).__init__()\n",
        "        \n",
        "        print('T_GMAT_module', n_heads, in_channel, num_nodes, dropout, alpha)\n",
        "        self.net = T_GMAT(n_heads, in_channel, num_nodes, dropout, alpha)\n",
        "\n",
        "        self.mlp_convs = nn.ModuleList()\n",
        "        self.mlp_bns = nn.ModuleList()\n",
        "        last_channel = 32\n",
        "        for out_channel in mlp:\n",
        "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            last_channel = out_channel\n",
        "        \n",
        "        self.mlp_convs2 = nn.ModuleList()\n",
        "        self.mlp_bns2 = nn.ModuleList()\n",
        "        last_channel = n_heads\n",
        "        for out_channel in mlp2:\n",
        "            self.mlp_convs2.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            last_channel = out_channel\n",
        "\n",
        "        self.norm2 = nn.LayerNorm([32, num_nodes, temporal_len-4])\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.mlp = (nn.Conv2d(32,32,(1,kern),dilation=(1,dilation_factor))) \n",
        "  \n",
        "    def forward(self,x):\n",
        "        \n",
        "        bs, ch, n, dim = x.size()\n",
        "        \n",
        "        x_input = x.permute(0,1,3,2)\n",
        "        x_input_cpy = x_input\n",
        "\n",
        "        #-------------relu(CNN)-------------#\n",
        "        for i, conv in enumerate(self.mlp_convs):\n",
        "            x_input = F.relu((conv(x_input)))\n",
        "        #-------------relu(CNN)-------------#\n",
        "\n",
        "        #-------------GAT-------------#\n",
        "        x_input_cpy2 = x_input\n",
        "\n",
        "        x_input = self.net(x_input)\n",
        "\n",
        "        x_input = x_input_cpy2+ self.dropout1(x_input)\n",
        "        #-------------GAT-------------#\n",
        "\n",
        "        #-------------relu(CNN)-------------#\n",
        "        for i, conv in enumerate(self.mlp_convs2):\n",
        "          x_input = F.relu((conv(x_input)))\n",
        "        #-------------relu(CNN)-------------#\n",
        "\n",
        "        x_input = (x_input_cpy + self.dropout2(x_input)).permute(0,1,3,2)\n",
        "\n",
        "\n",
        "        #最後一維度緊收\n",
        "        x_input = F.relu(self.mlp(x_input))\n",
        "        x_input = self.norm2(x_input)\n",
        "\n",
        "        return x_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2B05e4OWtiW"
      },
      "source": [
        "### S-MGAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "2SImBCFlWu-7"
      },
      "outputs": [],
      "source": [
        "\n",
        "class S_GMAT_base(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, bias=True):\n",
        "        super(S_GMAT_base, self).__init__()\n",
        "\n",
        "        print('S_GMAT_base', n_heads, in_channel, num_nodes, dropout)\n",
        "        self.n_head = n_heads\n",
        "        self.f_in = num_nodes\n",
        "        self.a_src = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "        self.a_dst = nn.Parameter(torch.Tensor(self.n_head, num_nodes, 1))\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(num_nodes))\n",
        "            nn.init.constant_(self.bias, 0)\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "        nn.init.xavier_uniform_(self.a_src, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a_dst, gain=1.414)\n",
        "        \n",
        "        self.W_si_1 = nn.Parameter(torch.zeros(size=(n_heads, 1, 1))).cuda()\n",
        "        nn.init.xavier_uniform_(self.W_si_1.data, gain=1.414)\n",
        "        \n",
        "        self.W_ei_1 = nn.Parameter(torch.zeros(size=(n_heads, 1, 1))).cuda()\n",
        "        nn.init.xavier_uniform_(self.W_ei_1.data, gain=1.414)\n",
        "       \n",
        "    def forward(self, h, adj):\n",
        "\n",
        "        bs, ch, n, dim = h.size()\n",
        "        \n",
        "        attn_src = torch.matmul(h, self.a_src)\n",
        "        attn_dst = torch.matmul(h, self.a_dst)\n",
        "        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(\n",
        "            0, 1, 3, 2\n",
        "        )\n",
        "        attn = self.leaky_relu(attn)\n",
        "        zero_vec = -9e15*torch.ones_like(attn)\n",
        "        attn = torch.where(adj > 0, attn, zero_vec) \n",
        "\n",
        "        attn = abs(self.W_si_1)*attn+abs(self.W_ei_1)*adj\n",
        "        \n",
        "        attn = self.softmax(attn) # bs x n_head x n x n\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        alpha = 0.05\n",
        "        all = [h]\n",
        "        h_prime = h\n",
        "        h_prime = alpha*h+ (1-alpha)* torch.matmul(attn, h_prime)\n",
        "        all.append(h_prime)\n",
        "        h_prime = alpha*h+ (1-alpha)* torch.matmul(attn, h_prime)\n",
        "        all.append(h_prime)\n",
        "\n",
        "        return torch.cat(all, dim=1)\n",
        "class S_GMAT(nn.Module):\n",
        "    def __init__(self, n_heads, in_channel, num_nodes, dropout, alpha):\n",
        "        super(S_GMAT, self).__init__()\n",
        "        \n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.layer = S_GMAT_base(\n",
        "                    n_heads, in_channel, num_nodes, dropout\n",
        "                )\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        bs,ch,n,dim = x.size()\n",
        "\n",
        "        x = self.layer(x, adj)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class S_GMAT_module(nn.Module):\n",
        "    def __init__(self, depth, temporal_len, n_heads, in_channel, num_nodes, mlp, mlp2, dropout, alpha):\n",
        "        super(S_GMAT_module, self).__init__()\n",
        "        \n",
        "        print('S_GMAT_module', n_heads, in_channel, num_nodes, dropout, alpha)\n",
        "\n",
        "        self.gat_net1 = S_GMAT(n_heads, in_channel, temporal_len, dropout, alpha)\n",
        "        self.gat_net2 = S_GMAT(n_heads, in_channel, temporal_len, dropout, alpha)\n",
        "\n",
        "        self.mlp_convs_start_1 = nn.Conv2d(in_channel, n_heads, 1)\n",
        "        self.mlp_convs_start_2 = nn.Conv2d(in_channel, n_heads, 1)\n",
        "\n",
        "        self.mlp_convs_end_1 = nn.Conv2d(n_heads*(1+depth), 32, 1)\n",
        "        self.mlp_convs_end_2 = nn.Conv2d(n_heads*(1+depth), 32, 1)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "        self.m1 = nn.GroupNorm((1+depth), n_heads*(1+depth))\n",
        "        self.m2 = nn.GroupNorm((1+depth), n_heads*(1+depth))\n",
        "\n",
        "        self.mlp1 = (nn.Conv2d(32,32,(1,1))) \n",
        "        self.mlp2 = (nn.Conv2d(32,32,(1,1))) \n",
        "\n",
        "        self.norm1 = nn.LayerNorm([32, num_nodes, temporal_len])\n",
        "        self.norm2 = nn.LayerNorm([32, num_nodes, temporal_len])\n",
        "\n",
        "    def forward(self,x,adj1,adj2):\n",
        "        \n",
        "        bs, ch, n, dim = x.size()\n",
        "        \n",
        "        x_input = x.clone()\n",
        "        #(CNN1)\n",
        "        x_input = self.mlp_convs_start_1(x_input)\n",
        "\n",
        "        #(GMAT)\n",
        "        x_input = F.elu(self.gat_net1(x_input,adj1))\n",
        "\n",
        "        #(CNN2)\n",
        "        x_input = self.mlp_convs_end_1(x_input)\n",
        "        \n",
        "        x_input = (x + self.dropout1(x_input))\n",
        "\n",
        "        #最後MLP\n",
        "        x_input = F.elu(self.mlp1(x_input))\n",
        "        x_input1 = self.norm1(x_input)\n",
        "\n",
        "        #--------------------------------------------------#\n",
        "\n",
        "        x_input = x.clone()\n",
        "        #(CNN1)\n",
        "        x_input = self.mlp_convs_start_2(x_input)\n",
        "\n",
        "        #(GMAT)\n",
        "        x_input = F.elu(self.gat_net2(x_input,adj2))\n",
        "\n",
        "        #(CNN2)\n",
        "        x_input = self.mlp_convs_end_2(x_input)\n",
        "        \n",
        "        x_input = (x + self.dropout2(x_input))\n",
        "\n",
        "        x_input = F.elu(self.mlp2(x_input))\n",
        "        x_input2 = self.norm2(x_input)\n",
        "        \n",
        "        x_input = x_input1 + x_input2\n",
        "        \n",
        "        return x_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF2qb8zTVa1Q"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IovaqRlhVa1Q"
      },
      "outputs": [],
      "source": [
        "class GMAT_Net(nn.Module):\n",
        "    def __init__(self, \n",
        "                 model_type, \n",
        "                 num_nodes, \n",
        "                 device, \n",
        "                 predefined_A=None, \n",
        "                 dropout=0.3, \n",
        "                 conv_channels=32, \n",
        "                 residual_channels=32, \n",
        "                 skip_channels=64, \n",
        "                 end_channels=128, \n",
        "                 seq_length=12, \n",
        "                 in_dim=2, \n",
        "                 out_dim=12, \n",
        "                 layers=3, \n",
        "                 layer_norm_affline=True):\n",
        "        super(GMAT_Net, self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.num_nodes = num_nodes\n",
        "        self.dropout = dropout\n",
        "        self.predefined_A = predefined_A\n",
        "        self.layers = layers\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        self.residual_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "        self.norm = nn.ModuleList()\n",
        "        self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
        "                                    out_channels=residual_channels,\n",
        "                                    kernel_size=(1, 1))\n",
        "        \n",
        "        self.f_gmat_list = nn.ModuleList()\n",
        "        in_channel = 32\n",
        "        n_heads = 8\n",
        "        dropout = 0\n",
        "        alpha = 0.2\n",
        "        self.f_gmat_list.append(\n",
        "            F_GMAT_module(\n",
        "              n_heads=n_heads, in_channel= in_channel, num_nodes=num_nodes, mlp=[n_heads],mlp2=[32], dropout=dropout, alpha=alpha\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.t_gmat_list_1 = nn.ModuleList()\n",
        "        self.t_gmat_list_2 = nn.ModuleList()\n",
        "\n",
        "        self.s_gmat_list = nn.ModuleList() # dual GMAT Blocks\n",
        "        \n",
        "        # Modified from: https://github.com/nnzhan/MTGNN\n",
        "        kernel_size = 7\n",
        "        dilation_exponential = 1\n",
        "        if dilation_exponential>1:\n",
        "            self.receptive_field = int(1+(kernel_size-1)*(dilation_exponential**layers-1)/(dilation_exponential-1))\n",
        "        else:\n",
        "            self.receptive_field = layers*(kernel_size-1) + 1\n",
        "        \n",
        "        print(\"# Model Type\", self.model_type)\n",
        "        print(\"# receptive_field\", self.receptive_field)\n",
        "        self.receptive_field = 13\n",
        "        i=0\n",
        "        if dilation_exponential>1:\n",
        "            rf_size_i = int(1 + i*(kernel_size-1)*(dilation_exponential**layers-1)/(dilation_exponential-1))\n",
        "        else:\n",
        "            rf_size_i = i*layers*(kernel_size-1)+1\n",
        "        new_dilation = 1\n",
        "        target_len = self.receptive_field\n",
        "\n",
        "        for j in range(1,layers+1):\n",
        "           \n",
        "            if dilation_exponential > 1:\n",
        "                rf_size_j = int(rf_size_i + (kernel_size-1)*(dilation_exponential**j-1)/(dilation_exponential-1))\n",
        "            else:\n",
        "                rf_size_j = rf_size_i+j*(kernel_size-1)\n",
        "\n",
        "            dilation_factor = 1\n",
        "            kern = 5\n",
        "\n",
        "            in_channel = 32\n",
        "            n_heads = 8\n",
        "            dropout = 0\n",
        "            alpha = 0.2\n",
        "            self.t_gmat_list_1.append(\n",
        "                T_GMAT_module(\n",
        "                  kern= kern, dilation_factor=dilation_factor, temporal_len = target_len, n_heads=n_heads, in_channel= in_channel, num_nodes=num_nodes, mlp=[n_heads],mlp2=[32], dropout=dropout, alpha=alpha\n",
        "                )\n",
        "            )\n",
        "            \n",
        "            self.t_gmat_list_2.append(\n",
        "                T_GMAT_module(\n",
        "                  kern= kern, dilation_factor=dilation_factor, temporal_len = target_len, n_heads=n_heads, in_channel= in_channel, num_nodes=num_nodes, mlp=[n_heads],mlp2=[32], dropout=dropout, alpha=alpha\n",
        "                )\n",
        "            )\n",
        "            \n",
        "            target_len -= 4\n",
        "\n",
        "            in_channel = 32\n",
        "            n_heads = 8\n",
        "            dropout = 0\n",
        "            alpha = 0.2\n",
        "            \n",
        "            depth = 2\n",
        "            self.s_gmat_list.append(\n",
        "                S_GMAT_module(\n",
        "                  depth=depth, temporal_len = target_len, n_heads=n_heads, in_channel= in_channel, num_nodes=num_nodes, mlp=[n_heads],mlp2=[32], dropout=dropout, alpha=alpha\n",
        "                )\n",
        "            )\n",
        "            # 1x1 convolution for skip connection\n",
        "            self.skip_convs.append(nn.Conv2d(in_channels=conv_channels,\n",
        "                                                out_channels=skip_channels,\n",
        "                                                kernel_size=(1, target_len)))\n",
        "            \n",
        "            self.norm.append(LayerNorm((residual_channels, num_nodes, target_len),elementwise_affine=layer_norm_affline))\n",
        "            \n",
        "            new_dilation *= dilation_exponential\n",
        "    \n",
        "        self.end_conv_1 = nn.Conv2d(in_channels=skip_channels,\n",
        "                                             out_channels=end_channels,\n",
        "                                             kernel_size=(1,1),\n",
        "                                             bias=True)\n",
        "        self.end_conv_2 = nn.Conv2d(in_channels=end_channels,\n",
        "                                             out_channels=out_dim,\n",
        "                                             kernel_size=(1,1),\n",
        "                                             bias=True)\n",
        "\n",
        "        if self.seq_length > self.receptive_field:\n",
        "            self.skip0 = nn.Conv2d(in_channels=in_dim, out_channels=skip_channels, kernel_size=(1, self.seq_length), bias=True)\n",
        "            self.skipE = nn.Conv2d(in_channels=residual_channels, out_channels=skip_channels, kernel_size=(1, self.seq_length-self.receptive_field+1), bias=True)\n",
        "\n",
        "        else:\n",
        "            self.skip0 = nn.Conv2d(in_channels=in_dim, out_channels=skip_channels, kernel_size=(1, self.receptive_field), bias=True)\n",
        "            self.skipE = nn.Conv2d(in_channels=residual_channels, out_channels=skip_channels, kernel_size=(1, 1), bias=True)\n",
        "\n",
        "        self.idx = torch.arange(self.num_nodes).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, input, input_1,input_2,input_3,input_4, idx=None):\n",
        "        seq_len = input.size(3)\n",
        "        assert seq_len==self.seq_length, 'input sequence length not equal to preset sequence length'\n",
        "\n",
        "        # Step0: 檢查receptive_field, 不足則padding0\n",
        "        if self.seq_length<self.receptive_field:\n",
        "            input = nn.functional.pad(input,(self.receptive_field-self.seq_length,0,0,0))\n",
        "            input_1 = nn.functional.pad(input_1,(self.receptive_field-self.seq_length,0,0,0))\n",
        "            input_2 = nn.functional.pad(input_2,(self.receptive_field-self.seq_length,0,0,0))\n",
        "            input_4 = nn.functional.pad(input_4,(self.receptive_field-self.seq_length,0,0,0))\n",
        "            \n",
        "            input_3 = nn.functional.pad(input_3,(self.receptive_field-self.seq_length,0,0,0))\n",
        "\n",
        "\n",
        "        # Step1: turn([64, 2, 207, 13]) to ([64, 32, 207, 13]) => 固定用同一conv\n",
        "        x = self.start_conv(input) \n",
        "        x_1 = self.start_conv(input_1)  \n",
        "        x_2 = self.start_conv(input_2)\n",
        "        \n",
        "        x_4 = self.start_conv(input_4) \n",
        "        x_3 = self.start_conv(input_3)  \n",
        "\n",
        "        x = self.f_gmat_list[0](x,x_1,x_2,x_3,x_4)\n",
        "\n",
        "        skip = self.skip0(F.dropout(input, self.dropout, training=self.training))\n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            residual = x    \n",
        "            \n",
        "            filter = self.t_gmat_list_1[i](x)\n",
        "            filter = torch.tanh(filter)\n",
        "\n",
        "            gate = self.t_gmat_list_2[i](x)\n",
        "            gate = torch.sigmoid(gate)\n",
        "\n",
        "            x = filter * gate\n",
        "            x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "            s = x\n",
        "            \n",
        "            s = self.skip_convs[i](s)    \n",
        "\n",
        "            skip = s + skip\n",
        "            # Two GMAT Block of different directions implemented in S_GMAT_module \n",
        "            x = self.s_gmat_list[i](x, self.predefined_A[0], self.predefined_A[1])\n",
        "\n",
        "            x = x + residual[:, :, :, -x.size(3):]\n",
        "            x = self.norm[i](x,self.idx)\n",
        "            \n",
        "        skip = self.skipE(x) + skip\n",
        "        x = F.relu(skip)\n",
        "        x = F.relu(self.end_conv_1(x))\n",
        "        x = self.end_conv_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbowdbREANli"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTga9hrXANli"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, lrate, wdecay, clip, step_size, seq_out_len, scaler, device, cl=True):\n",
        "        self.scaler = scaler\n",
        "        self.model = model\n",
        "        self.model.to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
        "        self.loss = masked_mae\n",
        "        self.clip = clip\n",
        "        self.step = step_size\n",
        "        self.iter = 1\n",
        "        self.task_level = 1\n",
        "        self.seq_out_len = seq_out_len\n",
        "        self.cl = cl\n",
        "\n",
        "    def train(self, input, input_1, input_2, input_3, input_4 ,real_val, idx=None):\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(input, input_1, input_2, input_3,input_4, idx=idx)\n",
        "        output = output.transpose(1,3)\n",
        "        real = torch.unsqueeze(real_val,dim=1)\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "        \n",
        "        if self.iter%self.step==0 and self.task_level<=self.seq_out_len:\n",
        "            self.task_level +=1\n",
        "            print(\"### cl learning\\n iter\",self.iter,\"\\niter%step\",self.iter%self.step,\"\\ntask_level\",self.task_level)\n",
        "            print(\"# predict len:\", len(predict[:, :, :, :self.task_level]))\n",
        "        \n",
        "        if self.cl:\n",
        "            loss = masked_mae(predict[:, :, :, :self.task_level], real[:, :, :, :self.task_level], 0.0)\n",
        "        else:\n",
        "            loss = masked_mae(predict, real, 0.0)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        if self.clip is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
        "\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        metrics = metric(predict, real) # mae,mape,rmse\n",
        "        \n",
        "        self.iter += 1\n",
        "        return metrics # mae,mape,rmse\n",
        "\n",
        "    def eval(self, input, input_1, input_2, input_3,input_4, real_val):\n",
        "        self.model.eval()\n",
        "        output = self.model(input, input_1, input_2, input_3,input_4)\n",
        "        output = output.transpose(1,3)\n",
        "        real = torch.unsqueeze(real_val,dim=1)\n",
        "        predict = self.scaler.inverse_transform(output)\n",
        "        \n",
        "        metrics = metric(predict, real) # mae,mape,rmse\n",
        "        return metrics # mae,mape,rmse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_84y8rqsANlj"
      },
      "source": [
        "### Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQX_bh_FANlj",
        "outputId": "40e5b783-fd35-47ee-aa8e-ee9f1773e5b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# args Namespace(adj_data='../Data/RMThsin/adj_mat_RMThsin.pkl', adjtype='doubletransition', batch_size=64, cl=True, clip=5, conv_channels=32, data='../Data/RMThsin', device='cuda', dropout=0.5, end_channels=128, epochs=200, expid=202212010003, in_dim=2, layers=3, learning_rate=0.0005, log_print=False, model_type='GMAT_Net', num_nodes=11, print_every=50, residual_channels=32, runs=3, save='./save/', seed=101, seq_in_len=12, seq_out_len=12, skip_channels=64, step_size1=1500, step_size2=100, weight_decay=0.0001)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def str_to_bool(value):\n",
        "    if isinstance(value, bool):\n",
        "        return value\n",
        "    if value.lower() in {'false', 'f', '0', 'no', 'n'}:\n",
        "        return False\n",
        "    elif value.lower() in {'true', 't', '1', 'yes', 'y'}:\n",
        "        return True\n",
        "    raise ValueError(f'{value} is not a valid boolean value')\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model_type',type=str,default='GMAT_Net',help='model type')\n",
        "\n",
        "parser.add_argument('--device',type=str,default='cuda',help='')\n",
        "parser.add_argument('--adjtype',type=str,default='doubletransition',help='adj type')\n",
        "parser.add_argument('--cl', type=str_to_bool, default=True,help='whether to do curriculum learning')\n",
        "parser.add_argument('--conv_channels',type=int,default=32,help='convolution channels')\n",
        "parser.add_argument('--residual_channels',type=int,default=32,help='residual channels')\n",
        "parser.add_argument('--in_dim',type=int,default=2,help='inputs dimension')\n",
        "parser.add_argument('--seq_in_len',type=int,default=12,help='input sequence length')\n",
        "parser.add_argument('--seq_out_len',type=int,default=12,help='output sequence length')\n",
        "parser.add_argument('--batch_size',type=int,default=64,help='batch size')\n",
        "parser.add_argument('--clip',type=int,default=5,help='clip')\n",
        "\n",
        "\n",
        "parser.add_argument('--skip_channels',type=int,default=64,help='skip channels')\n",
        "parser.add_argument('--end_channels',type=int,default=128,help='end channels')\n",
        "parser.add_argument('--layers',type=int,default=3,help='number of layers')\n",
        "\n",
        "parser.add_argument('--print_every',type=int,default=50,help='')\n",
        "parser.add_argument('--seed',type=int,default=101,help='random seed')\n",
        "parser.add_argument('--save',type=str,default='./save/',help='save path')\n",
        "\n",
        "parser.add_argument('--log_print', type=str_to_bool, default=False ,help='whether to load static feature')\n",
        "\n",
        "parser.add_argument('--learning_rate',type=float,default=0.0005,help='learning rate')\n",
        "parser.add_argument('--weight_decay',type=float,default=0.0001,help='weight decay rate')\n",
        "parser.add_argument('--dropout',type=float,default=0.5,help='dropout rate')\n",
        "\n",
        "target = 'RMThsin'\n",
        "parser.add_argument('--data',type=str,default='../Data/'+target ,help='data path')\n",
        "parser.add_argument('--adj_data',type=str,default='../Data/'+target+'/adj_mat_'+target+'.pkl',help='adj data path')\n",
        "parser.add_argument('--num_nodes',type=int,default=11,help='number of nodes/variables')\n",
        "parser.add_argument('--step_size1',type=int,default=1500,help='step_size')\n",
        "parser.add_argument('--step_size2',type=int,default=100,help='step_size')\n",
        "\n",
        "parser.add_argument('--expid',type=int,default=202212010003,help='experiment id')\n",
        "parser.add_argument('--runs',type=int,default=3,help='number of runs')\n",
        "parser.add_argument('--epochs',type=int,default=200,help='')\n",
        "\n",
        "torch.set_num_threads(3)\n",
        "\n",
        "args=parser.parse_args(args=[])\n",
        "print('# args', args)\n",
        "\n",
        "device = torch.device(args.device)\n",
        "\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRD8IwnmANlj"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUl-Eg70ANlj",
        "outputId": "fe6a2181-0c6d-4aae-c561-928241ad1f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "range_type \n",
            "loading: train -> ../Data/RMThsin train.npz\n",
            "loading: val -> ../Data/RMThsin val.npz\n",
            "loading: test -> ../Data/RMThsin test.npz\n",
            "dict_keys(['x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test', 'scaler'])\n",
            "data['x_' + key]: x_train\n",
            "data['x_' + key]: x_val\n",
            "data['x_' + key]: x_test\n",
            "range_type ma3\n",
            "loading: train_ma3 -> ../Data/RMThsin train_ma3.npz\n",
            "loading: val_ma3 -> ../Data/RMThsin val_ma3.npz\n",
            "loading: test_ma3 -> ../Data/RMThsin test_ma3.npz\n",
            "dict_keys(['x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test', 'scaler', 'x_train_1', 'y_train_1', 'x_val_1', 'y_val_1', 'x_test_1', 'y_test_1'])\n",
            "data['x_' + key]: x_train_1\n",
            "data['x_' + key]: x_val_1\n",
            "data['x_' + key]: x_test_1\n",
            "range_type ma6\n",
            "loading: train_ma6 -> ../Data/RMThsin train_ma6.npz\n",
            "loading: val_ma6 -> ../Data/RMThsin val_ma6.npz\n",
            "loading: test_ma6 -> ../Data/RMThsin test_ma6.npz\n",
            "dict_keys(['x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test', 'scaler', 'x_train_1', 'y_train_1', 'x_val_1', 'y_val_1', 'x_test_1', 'y_test_1', 'x_train_2', 'y_train_2', 'x_val_2', 'y_val_2', 'x_test_2', 'y_test_2'])\n",
            "data['x_' + key]: x_train_2\n",
            "data['x_' + key]: x_val_2\n",
            "data['x_' + key]: x_test_2\n",
            "range_type ga12\n",
            "loading: train_ga12 -> ../Data/RMThsin train_ga12.npz\n",
            "loading: val_ga12 -> ../Data/RMThsin val_ga12.npz\n",
            "loading: test_ga12 -> ../Data/RMThsin test_ga12.npz\n",
            "dict_keys(['x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test', 'scaler', 'x_train_1', 'y_train_1', 'x_val_1', 'y_val_1', 'x_test_1', 'y_test_1', 'x_train_2', 'y_train_2', 'x_val_2', 'y_val_2', 'x_test_2', 'y_test_2', 'x_train_3', 'y_train_3', 'x_val_3', 'y_val_3', 'x_test_3', 'y_test_3'])\n",
            "data['x_' + key]: x_train_3\n",
            "data['x_' + key]: x_val_3\n",
            "data['x_' + key]: x_test_3\n",
            "range_type ga24\n",
            "loading: train_ga24 -> ../Data/RMThsin train_ga24.npz\n",
            "loading: val_ga24 -> ../Data/RMThsin val_ga24.npz\n",
            "loading: test_ga24 -> ../Data/RMThsin test_ga24.npz\n",
            "dict_keys(['x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test', 'scaler', 'x_train_1', 'y_train_1', 'x_val_1', 'y_val_1', 'x_test_1', 'y_test_1', 'x_train_2', 'y_train_2', 'x_val_2', 'y_val_2', 'x_test_2', 'y_test_2', 'x_train_3', 'y_train_3', 'x_val_3', 'y_val_3', 'x_test_3', 'y_test_3', 'x_train_4', 'y_train_4', 'x_val_4', 'y_val_4', 'x_test_4', 'y_test_4'])\n",
            "data['x_' + key]: x_train_4\n",
            "data['x_' + key]: x_val_4\n",
            "data['x_' + key]: x_test_4\n"
          ]
        }
      ],
      "source": [
        "\"\"\"### Loading Data\"\"\"\n",
        "\n",
        "batch_size = args.batch_size\n",
        "valid_batch_size = args.batch_size\n",
        "test_batch_size = args.batch_size\n",
        "data = {}\n",
        "\n",
        "feature_id = 0\n",
        "for range_type in ['','ma3','ma6','ga12','ga24']:\n",
        "    print(\"range_type\", range_type)\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        \n",
        "        if range_type == \"\":\n",
        "          category = category\n",
        "          key = category\n",
        "        else:\n",
        "          \n",
        "          key = category + \"_\" + str(feature_id)\n",
        "          category = category + \"_\" + range_type\n",
        "\n",
        "        # Loading npz \n",
        "        cat_data = np.load(os.path.join(args.data, category + '.npz'))\n",
        "        print(\"loading:\", category ,'->', args.data, category + '.npz')\n",
        "\n",
        "        data['x_' + key] = cat_data['x'][:]     # (?, 12, 207, 2)\n",
        "        data['y_' + key] = cat_data['y'][:]   # (?, 12, 207, 2)\n",
        "    \n",
        "    if range_type == '':\n",
        "        # 使用train的mean/std來正規化valid/test #\n",
        "        scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())\n",
        "        data['scaler'] = scaler\n",
        "\n",
        "    print(data.keys())\n",
        "    # 將欲訓練特徵改成正規化\n",
        "    for category in ['train', 'val', 'test']:\n",
        "        \n",
        "        if range_type == \"\":\n",
        "          key = category\n",
        "        else:\n",
        "          key = category + \"_\" + str(feature_id)\n",
        "\n",
        "        data['x_' + key][..., 0] = data['scaler'].transform(data['x_' + key][..., 0])\n",
        "        print(\"data['x_' + key]:\", 'x_' + key)\n",
        "\n",
        "    feature_id += 1\n",
        "\n",
        "#print(data['x_train'].shape)\n",
        "data['train_loader'] = DataLoaderM(\n",
        "    data['x_train'], data['y_train'], \n",
        "    data['x_train_1'], \n",
        "    data['x_train_2'], \n",
        "    data['x_train_3'],\n",
        "    data['x_train_4'],\n",
        "    batch_size)\n",
        "\n",
        "data['val_loader'] = DataLoaderM(\n",
        "    data['x_val'], data['y_val'], \n",
        "    data['x_val_1'],  \n",
        "    data['x_val_2'],  \n",
        "    data['x_val_3'],  \n",
        "    data['x_val_4'],  \n",
        "    valid_batch_size)\n",
        "\n",
        "data['test_loader'] = DataLoaderM(\n",
        "    data['x_test'], data['y_test'], \n",
        "    data['x_test_1'],  \n",
        "    data['x_test_2'], \n",
        "    data['x_test_3'],\n",
        "    data['x_test_4'],  \n",
        "    test_batch_size)\n",
        "\n",
        "sensor_ids, sensor_id_to_ind, adj_mx = load_adj(args.adj_data,args.adjtype)   # adjtype: default='doubletransition'\n",
        "\n",
        "adj_mx = [torch.tensor(i).to(device) for i in adj_mx]\n",
        "\n",
        "dataloader = data.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXEI2RAYVYPT"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkri3OflVYPY"
      },
      "outputs": [],
      "source": [
        "def main(runid):\n",
        "    \n",
        "\n",
        "    model = GMAT_Net(args.model_type, \n",
        "                   args.num_nodes,\n",
        "                   device, \n",
        "                   predefined_A=adj_mx,  \n",
        "                   dropout=args.dropout, \n",
        "                   conv_channels=args.conv_channels, \n",
        "                   residual_channels=args.residual_channels,\n",
        "                   skip_channels=args.skip_channels, \n",
        "                   end_channels= args.end_channels,\n",
        "                   seq_length=args.seq_in_len, \n",
        "                   in_dim=args.in_dim, \n",
        "                   out_dim=args.seq_out_len,\n",
        "                   layers=args.layers,  \n",
        "                   layer_norm_affline=True)\n",
        "\n",
        "    print(model)\n",
        "    nParams = sum([p.nelement() for p in model.parameters()])       # model參數量!\n",
        "\n",
        "    engine = Trainer(model, args.learning_rate, args.weight_decay, args.clip, args.step_size1, args.seq_out_len, data['scaler'], device, args.cl)\n",
        "    \n",
        "    print(\"start training...\",flush=True)\n",
        "    his_loss =[]\n",
        "    val_time = []\n",
        "    train_time = []\n",
        "    minl = 1e5\n",
        "    start_epoch=0\n",
        "    train_loss_epoch = []  # 紀錄train在epoch收斂\n",
        "    valid_loss_epoch = []  # 紀錄valid在epoch收斂\n",
        "    \n",
        "    for i in range(start_epoch,start_epoch+args.epochs+1):\n",
        "        train_mae = []\n",
        "        train_mape = []\n",
        "        train_rmse = []\n",
        "        t1 = time.time()\n",
        "        dataloader['train_loader'].shuffle()  # 為了檢視資料先拿掉\n",
        "        for iter, (x, y,x_1,x_2,x_3,x_4) in enumerate(dataloader['train_loader'].get_iterator()):\n",
        "            trainx = torch.Tensor(x).to(device)\n",
        "            trainx= trainx.transpose(1, 3)\n",
        "            trainy = torch.Tensor(y).to(device)\n",
        "            trainy = trainy.transpose(1, 3)\n",
        "            \n",
        "            trainx_1 = torch.Tensor(x_1).to(device)\n",
        "            trainx_1= trainx_1.transpose(1, 3)\n",
        "            \n",
        "            trainx_2 = torch.Tensor(x_2).to(device)\n",
        "            trainx_2= trainx_2.transpose(1, 3)\n",
        "            \n",
        "            \n",
        "            trainx_3 = torch.Tensor(x_3).to(device)\n",
        "            trainx_3= trainx_3.transpose(1, 3)\n",
        "            \n",
        "            trainx_4 = torch.Tensor(x_4).to(device)\n",
        "            trainx_4= trainx_4.transpose(1, 3)\n",
        "            \n",
        "            #mae,mape,rmse\n",
        "            metrics = engine.train(trainx,trainx_1,trainx_2,trainx_3,trainx_4 ,trainy[:,0,:,:])\n",
        "\n",
        "            train_mae.append(metrics[0])\n",
        "            train_mape.append(metrics[1])\n",
        "            train_rmse.append(metrics[2])\n",
        "\n",
        "            if iter % args.print_every == 0 :\n",
        "                log = 'Iter: {:03d}, Train MAE: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}'\n",
        "                print(log.format(iter, train_mae[-1], train_mape[-1], train_rmse[-1],flush=True))\n",
        "        t2 = time.time()\n",
        "        train_time.append(t2-t1)\n",
        "        #validation\n",
        "        valid_mae = []\n",
        "        valid_mape = []\n",
        "        valid_rmse = []\n",
        "\n",
        "        s1 = time.time()\n",
        "        for iter, (x, y,x_1,x_2,x_3,x_4)  in enumerate(dataloader['val_loader'].get_iterator()):\n",
        "            testx = torch.Tensor(x).to(device)\n",
        "            testx = testx.transpose(1, 3)\n",
        "            testy = torch.Tensor(y).to(device)\n",
        "            testy = testy.transpose(1, 3)\n",
        "            \n",
        "            testx_1 = torch.Tensor(x_1).to(device)\n",
        "            testx_1= testx_1.transpose(1, 3)\n",
        "            \n",
        "            testx_2 = torch.Tensor(x_2).to(device)\n",
        "            testx_2= testx_2.transpose(1, 3)\n",
        "            \n",
        "            \n",
        "            testx_3 = torch.Tensor(x_3).to(device)\n",
        "            testx_3= testx_3.transpose(1, 3)\n",
        "            \n",
        "            testx_4 = torch.Tensor(x_4).to(device)\n",
        "            testx_4= testx_4.transpose(1, 3)\n",
        "            \n",
        "            \n",
        "            metrics = engine.eval(testx, testx_1,testx_2,testx_3,testx_4, testy[:,0,:,:])\n",
        "            valid_mae.append(metrics[0])\n",
        "            valid_mape.append(metrics[1])\n",
        "            valid_rmse.append(metrics[2])\n",
        "            \n",
        "        s2 = time.time()\n",
        "        log = 'Epoch: {:03d}, Inference Time: {:.4f} secs'\n",
        "        print(log.format(i,(s2-s1)))\n",
        "        val_time.append(s2-s1)\n",
        "        mtrain_mae = np.mean(train_mae)\n",
        "        mtrain_mape = np.mean(train_mape)\n",
        "        mtrain_rmse = np.mean(train_rmse)\n",
        "\n",
        "        mvalid_mae = np.mean(valid_mae)\n",
        "        mvalid_mape = np.mean(valid_mape)\n",
        "        mvalid_rmse = np.mean(valid_rmse)\n",
        "        \n",
        "        his_loss.append(mvalid_mae)\n",
        "\n",
        "        log = 'Epoch: {:03d}, Train MAE: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f},  Valid MAE: {:.4f}, Valid MAPE: {:.4f}, Valid RMSE: {:.4f}, Training Time: {:.4f}/epoch'\n",
        "        print(log.format(i, mtrain_mae, mtrain_mape, mtrain_rmse, mvalid_mae, mvalid_mape, mvalid_rmse, (t2 - t1)),flush=True)\n",
        "        \n",
        "        train_loss_epoch.append(mtrain_mae)\n",
        "        valid_loss_epoch.append(mvalid_mae)\n",
        "        \n",
        "        if mvalid_mae<minl:\n",
        "            target_best_model = args.save + \"exp\" + str(args.expid) + \"_\" + str(runid) +\".pth\"\n",
        "            print(\"### Update Best Model:\",target_best_model, '*LOSS:', mvalid_mae, \" ###\")\n",
        "            SAVE_PATH = args.save + \"exp\" + str(args.expid) + \"_\" + str(runid) +\".pth\"\n",
        "            torch.save({\n",
        "              'epoch': i,\n",
        "              'task_level': engine.task_level,\n",
        "              'model_state_dict': engine.model.state_dict(),\n",
        "              'optimizer_state_dict': engine.optimizer.state_dict(),\n",
        "              'loss': mvalid_mae,\n",
        "              'train_loss': train_loss_epoch,\n",
        "              'valid_loss': valid_loss_epoch\n",
        "            }, SAVE_PATH)\n",
        "            minl = mvalid_mae\n",
        "\n",
        "    print(\"Average Training Time: {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
        "    print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))\n",
        "\n",
        "\n",
        "    bestid = np.argmin(his_loss)\n",
        "    \n",
        "\n",
        "    print(\"Training finished\")\n",
        "    print(\"The valid loss on best model is\", str(round(his_loss[bestid],4)))\n",
        "    \n",
        "    target_model = args.save + \"exp\" + str(args.expid) + \"_\" + str(runid) +\".pth\"\n",
        "   \n",
        "    print(\"### loading model is:\",target_model ,'###')\n",
        "    \n",
        "    SAVE_PATH = args.save + \"exp\" + str(args.expid) + \"_\" + str(runid) +\".pth\"\n",
        "    checkpoint = torch.load(SAVE_PATH)\n",
        "    engine.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    engine.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    engine.task_level = checkpoint['task_level']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    train_loss_epoch = checkpoint['train_loss']\n",
        "    valid_loss_epoch = checkpoint['valid_loss']\n",
        "    #draw_plot_loss(checkpoint)\n",
        "\n",
        "    ### 測試讀取出的model ### \n",
        "    valid_mae = []  \n",
        "    valid_mape = [] \n",
        "    valid_rmse = [] \n",
        "    tmp_y = []\n",
        "    for iter, (x, y,x_1,x_2,x_3,x_4)  in enumerate(dataloader['val_loader'].get_iterator()):  \n",
        "        \n",
        "        testx = torch.Tensor(x).to(device)  \n",
        "        testx = testx.transpose(1, 3)   \n",
        "        testy = torch.Tensor(y).to(device)  \n",
        "        testy = testy.transpose(1, 3)   \n",
        "        \n",
        "        testx_1 = torch.Tensor(x_1).to(device)\n",
        "        testx_1= testx_1.transpose(1, 3)\n",
        "\n",
        "        testx_2 = torch.Tensor(x_2).to(device)\n",
        "        testx_2= testx_2.transpose(1, 3)\n",
        "\n",
        "        testx_3 = torch.Tensor(x_3).to(device)\n",
        "        testx_3= testx_3.transpose(1, 3)\n",
        "        \n",
        "        testx_4 = torch.Tensor(x_4).to(device)\n",
        "        testx_4= testx_4.transpose(1, 3)\n",
        "\n",
        "        metrics = engine.eval(testx, testx_1,testx_2, testx_3,testx_4,testy[:,0,:,:]) \n",
        "        valid_mae.append(metrics[0])    \n",
        "        valid_mape.append(metrics[1])   \n",
        "        valid_rmse.append(metrics[2])   \n",
        "\n",
        "\n",
        "    mvalid_mae = np.mean(valid_mae) \n",
        "    mvalid_mape = np.mean(valid_mape)   \n",
        "    mvalid_rmse = np.mean(valid_rmse)   \n",
        "\n",
        "    print(\"### 2-The valid loss on loding model is\", str(round(mvalid_mae,4)))\n",
        "    minl= valid_mae   \n",
        "    print(\"### minl:\",minl, \"checkpoint['loss']:\",checkpoint['loss'])   \n",
        "    ### 測試讀取出的model ### \n",
        "\n",
        "    #valid data\n",
        "    outputs = []\n",
        "    realy = torch.Tensor(dataloader['y_val']).to(device)\n",
        "    \n",
        "    realy = realy.transpose(1,3)[:,0,:,:]\n",
        "    print('#realy', realy.shape)\n",
        "    \n",
        "    for iter, (x, y,x_1,x_2,x_3,x_4)  in enumerate(dataloader['val_loader'].get_iterator()):\n",
        "        testx = torch.Tensor(x).to(device)\n",
        "        testx = testx.transpose(1,3)\n",
        "        \n",
        "        testx_1 = torch.Tensor(x_1).to(device)\n",
        "        testx_1= testx_1.transpose(1, 3)\n",
        "\n",
        "        testx_2 = torch.Tensor(x_2).to(device)\n",
        "        testx_2= testx_2.transpose(1, 3)\n",
        "\n",
        "        testx_3 = torch.Tensor(x_3).to(device)\n",
        "        testx_3= testx_3.transpose(1, 3)\n",
        "\n",
        "        testx_4 = torch.Tensor(x_4).to(device)\n",
        "        testx_4= testx_4.transpose(1, 3)\n",
        "        with torch.no_grad():\n",
        "            preds = engine.model(testx,testx_1,testx_2,testx_3,testx_4)\n",
        "            preds = preds.transpose(1,3)  # 64,1,6,12\n",
        "\n",
        "        outputs.append(preds.squeeze()) # 64,1,6,12 ->squeeze()->64,6,12\n",
        "\n",
        "    yhat = torch.cat(outputs,dim=0)\n",
        "    yhat = yhat[:realy.size(0),...]  # 5240,6,12\n",
        "    print('# cat valid preds', yhat.shape)\n",
        "\n",
        "    pred = dataloader['scaler'].inverse_transform(yhat)\n",
        "    \n",
        "    vmae, vmape, vrmse  = metric(pred,realy)\n",
        "    print(\"valid - vmae, vmape, vrmse \", vmae, vmape, vrmse )\n",
        "    #----------------------------------#\n",
        "    #test data\n",
        "    outputs = []\n",
        "    realy = torch.Tensor(dataloader['y_test']).to(device)\n",
        "    realy = realy.transpose(1, 3)[:, 0, :, :]\n",
        "\n",
        "    for iter, (x, y,x_1,x_2,x_3,x_4)  in enumerate(dataloader['test_loader'].get_iterator()):\n",
        "        testx = torch.Tensor(x).to(device)\n",
        "        testx = testx.transpose(1, 3)\n",
        "        \n",
        "        testx_1 = torch.Tensor(x_1).to(device)\n",
        "        testx_1= testx_1.transpose(1, 3)\n",
        "\n",
        "        testx_2 = torch.Tensor(x_2).to(device)\n",
        "        testx_2= testx_2.transpose(1, 3)\n",
        "\n",
        "        testx_3 = torch.Tensor(x_3).to(device)\n",
        "        testx_3= testx_3.transpose(1, 3)\n",
        "\n",
        "        testx_4 = torch.Tensor(x_4).to(device)\n",
        "        testx_4= testx_4.transpose(1, 3)\n",
        "\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            preds = engine.model(testx,testx_1,testx_2,testx_3,testx_4)\n",
        "            preds = preds.transpose(1, 3)\n",
        "        outputs.append(preds.squeeze())\n",
        "\n",
        "    yhat = torch.cat(outputs, dim=0)\n",
        "    yhat = yhat[:realy.size(0), ...]  #10478, 6, 12\n",
        "    print('# cat test preds', yhat.shape)\n",
        "    \n",
        "    mae = []\n",
        "    mape = []\n",
        "    rmse = []\n",
        "    \n",
        "    for i in range(args.seq_out_len):\n",
        "        pred = dataloader['scaler'].inverse_transform(yhat[:, :, i])\n",
        "        \n",
        "        real = realy[:, :, i]\n",
        "\n",
        "        metrics = metric(pred, real)\n",
        "        \n",
        "        log = 'Evaluate best model on test data for horizon {:d}, Test MAE: {:.4f}, Test MAPE: {:.4f}, Test RMSE: {:.4f}'\n",
        "        print(log.format(i + 1, metrics[0], metrics[1], metrics[2]))\n",
        "        mae.append(metrics[0])\n",
        "        mape.append(metrics[1])\n",
        "        rmse.append(metrics[2])\n",
        "        \n",
        "    #sys.exit()\n",
        "    log = '{:.2f}   {:.2f}    {:.4f}  '\n",
        "    print(\"#### Final Results:\")\n",
        "    print(  str(args.expid) + \"_\" + str(runid)+'    ', \n",
        "          log.format(mae[0], rmse[0],   mape[0]),\n",
        "          log.format(mae[2], rmse[2],  mape[2]),\n",
        "          log.format(mae[5], rmse[5],   mape[5]),\n",
        "          log.format(mae[11], rmse[11],  mape[11]),\n",
        "         )\n",
        "    return vmae, vmape, vrmse, mae, mape, rmse\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    vmae = []\n",
        "    vmape = []\n",
        "    vrmse = []\n",
        "\n",
        "    mae = []\n",
        "    mape = []\n",
        "    rmse = []\n",
        "\n",
        "    for i in range(args.runs):\n",
        "        vm1, vm2, vm3, m1, m2, m3 = main(i)\n",
        "        vmae.append(vm1)\n",
        "        vmape.append(vm2)\n",
        "        vrmse.append(vm3)\n",
        "\n",
        "        mae.append(m1)\n",
        "        mape.append(m2)\n",
        "        rmse.append(m3)\n",
        "\n",
        "\n",
        "    mae = np.array(mae)\n",
        "    mape = np.array(mape)\n",
        "    rmse = np.array(rmse)\n",
        "\n",
        "    amae = np.mean(mae,0)\n",
        "    amape = np.mean(mape,0)\n",
        "    armse = np.mean(rmse,0)\n",
        "\n",
        "    smae = np.std(mae,0)\n",
        "    s_mape = np.std(mape,0)\n",
        "    srmse = np.std(rmse,0)\n",
        "\n",
        "\n",
        "    print('\\n\\nResults for 10 runs\\n\\n')\n",
        "    #valid data\n",
        "    print('valid\\tMAE\\tRMSE\\tMAPE')\n",
        "    log = 'mean:\\t{:.4f}\\t{:.4f}\\t{:.4f}'\n",
        "    print(log.format(np.mean(vmae),np.mean(vrmse),np.mean(vmape)))\n",
        "    log = 'std:\\t{:.4f}\\t{:.4f}\\t{:.4f}'\n",
        "    print(log.format(np.std(vmae),np.std(vrmse),np.std(vmape)))\n",
        "    print('\\n\\n')\n",
        "    #test data\n",
        "    print('test|horizon\\tMAE-mean\\tRMSE-mean\\tMAPE-mean\\tMAE-std\\tRMSE-std\\tMAPE-std')\n",
        "    for i in [2,5,11]:\n",
        "        log = '{:d}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\\t{:.4f}'\n",
        "        print(log.format(i+1, amae[i], armse[i], amape[i], smae[i], srmse[i], s_mape[i]))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}